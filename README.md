# LunarStudio â€“ Lightweight Local AI Desktop App (C++ + Qt + llama.cpp)

LunarStudio is a fast, privacyâ€‘focused offline AI assistant built using **C++**, **Qt/QML**, **FAISS**, **SQLite**, and **llama.cpp**.
It performs vector search, embedding generation, and LLM inference fully offline inside your system.

---

## ğŸš€ Features

- Fast local inference using GGUF models  
- Semantic search powered by FAISS  
- Sentence embeddings with MiniLM  
- Local database storage using SQLite  
- 100% offline & private  
- Lightweight compared to Electron or web view frameworks  

---

## ğŸ“‚ Project Structure

project/
â”œâ”€â”€ CMakeLists.txt
â”œâ”€â”€ src/
â”œâ”€â”€ include/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ all-MiniLM-L6-v2.F16.gguf
â”‚   â”œâ”€â”€ qwen2.5-0.5b-instruct-q8_0.gguf
â”œâ”€â”€ ic/
â”œâ”€â”€ build/
â””â”€â”€ README.md

---

## ğŸ“¥ Download Required Models

Before running LunarStudio, download the following models into the **models** folder.

### 1. All-MiniLM-L6-v2 (Embeddings Model)

cd models/
wget https://huggingface.co/leliuga/all-MiniLM-L6-v2-GGUF/resolve/main/all-MiniLM-L6-v2.F16.gguf

### 2. Qwen2.5 0.5B Instruct (Main LLM)

cd models/
wget https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct-GGUF/resolve/main/qwen2.5-0.5b-instruct-q8_0.gguf

---

## ğŸ”§ Building the Project

### Dependencies (example for Arch Linux)

sudo pacman -S qt6-base qt6-declarative qt6-tools cmake make gcc faiss sqlite openblas lapack nlohmann-json

### Build Steps

cd lunar-studio
mkdir build && cd build
cmake ..
make -j$(nproc)

---

## â–¶ï¸ Running

### CLI:
./localGPT


---

## ğŸ§  How It Works

1. Embeddings generated using MiniLM  
2. FAISS vector search  
3. Qwen2.5 LLM generates response  

---

## ğŸ›  Development

 (C++): src/*.cpp, include/*.hpp  

---

## ğŸ™Œ Contributing

PRs and issues are welcome!

---

